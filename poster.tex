\documentclass[landscape,final,a0paper,fontscale=0.277]{baposter}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}

\usepackage{multicol}

\usepackage{xypic}

% Multicol Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\columnsep}{1.5em}
\setlength{\columnseprule}{0mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Save space in lists. Use this after the opening of the list
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\compresslist}{%
\setlength{\itemsep}{1pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

\newcommand{\firstitem}{\item\vskip-5pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Here starts the poster
%%%---------------------------------------------------------------------------
%%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define some colors

\begin{poster}%
  % Poster Options
  {
  % Show grid to help with alignment
  grid=false,
  columns=3,
  % Column spacing
  colspacing=1em,
  % Color style
  bgColorOne=white,
  bgColorTwo=white,
  borderColor=black,
  headerColorOne=white,
  headerColorTwo=white,
  headerFontColor=black,
  boxColorOne=white,
  boxColorTwo=white,
  % Format of textbox
  textborder=rounded,
  % Format of text header
  eyecatcher=true,
  headerborder=closed,
  headerheight=0.1\textheight,
%  textfont=\sc, An example of changing the text font
  headershape=rounded,
  headershade=plain,
  headerfont=\Large\bf\textsc, %Sans Serif
  textfont={\setlength{\parindent}{1.5em}},
  boxshade=plain,
%  background=shade-tb,
  background=plain,
  linewidth=1pt
  }
  % Eye Catcher
  {} 
  % Title
  {\bf\textsc{Structured Statistical Code Prediction}\vspace{0.5em}}
  % Authors
  {\textsc{Cyrus Omar, Flavio Cruz, Salil Joshi}}
  % University logo
  {}
  %{% The makebox allows the title to flow into the logo, this is a hack because of the L shaped logo.
  %  \includegraphics[height=9.0em]{images/logo}
  %}

  \headerbox{Introduction}{name=intro, column=0, row = 0}{
    Our goal is to predict source code using both the \emph{semantics} of the programming language, and \emph{statistics} about typical usage scenarios. We created a model of source code that uses:
    \begin{itemize}\compresslist
        
      \item \textbf{Semantic Information} in two ways:
        \begin{itemize}\compresslist
          \firstitem We use the expected \emph{type} of an expression to constrian predictions. This is similar to (but more general than) what is done in existing IDE code completion systems
          \item We also condition on the \emph{syntactic context} i.e.\ the structure of the AST in which an expresion appears. For example, in $\texttt{int i =} \Box$ the hole ($\Box$) appears in an assignment context
        \end{itemize}
      \item \textbf{Statistical Information} by analyzing existing source code to build a probability distribution for expression while utilizing the language semantics
    \end{itemize}

    Code prediction is useful for code editors, but also for other specialized tools like enabling programmers with physical impairments to convey source code using input devices more limited than a keyboard
  }


  \headerbox{Model}{name=model, column=0, below = intro}{
    \begin{displaymath}
    \xymatrix{
      *+[F]{\txt{Type}} \ar[dr] \ar[drr] & &\\
      & *+[F]{\txt{Form}} \ar[r] & *+[F]{\txt{Expression}} \\
      *+[F]{\txt{Context}} \ar[ur] \ar[urr] & &
    }
    \end{displaymath}

    Our goal is to learn $\mathbf{P}(\textbf{expression}|\textbf{type},\textbf{context})$. \\We split this as:
    \begin{itemize}
      \item {$\mathbf{P}(\textbf{form} |\textbf{type},\textbf{context})$}. The forms an expression may take are:
        \begin{itemize}\compresslist
          \firstitem \vskip-5pt Literal
          \item Variable
          \item Method\compresslist
        \end{itemize}
      \item \vskip-5pt $\mathbf{P}(\textbf{expression} | \textbf{form})$. eg. if we already know we are using a literal, what is the probability we will use the given literal.
    \end{itemize}

    Then,
    \vskip-20pt
    \begin{align*}
      &\quad P(\text{expression} | \text{type}, \text{context}) =\\
      &\quad\quad P(\text{expression} | \text{form}) * P(\text{form} | \text{type}, \text{context})
      \end{align*}
  }

  \headerbox{Learning}{name=learning, column=1, row=0}{
    \setlength{\parindent}{0pt}
    We analyze existing code to learn the following parameters:
    \begin{itemize}
      \item $\mathbf{n}_{\mathbf{lit}}(l)$: For a literal $l$, $\#\{l | \text{type}, \text{context}\}$ i.e.\ the number of times $l$ was used in a given type and context
      \item $\mathbf{n}_{\mathbf{met}}(m)$ For a method $m$, $\#\{m | \text{type}, \text{context}\}$ i.e.\ the number of times $m$ was used in a given type and context
      \item $\mathbf{n}_{\mathbf{var}}$: $\#\{\text{variable} | \text{type}, \text{context}\}$ i.e.\ the number of times \emph{any} variable was used in a given type and context
      \item $\#\{\text{first use of a method } | \text{type}, \text{context}\}$ this is the number of times a previously unseen method was used.
      \item $\#\{\text{methods} | \text{type}\}$ i.e.\ the number of methods (used and unused) with the appropriate return type
    \end{itemize}
  }

  \headerbox{Prediction}{name=prediction, column=1, below=learning}{
    \setlength{\parindent}{0pt}
    $\mathbf{P}(\textbf{form} | \text{type}, \text{context})$ is a multinomial distribution, computed as:
    \begin{align*}
     &P(\text{form} = f | \text{type}, \text{context}) = \\
     &\quad \frac{\Sigma_{x \in f} n_{f}(x)}{( \Sigma_{l \in \text{literal}} n_{\text{lit}}(l) + \Sigma_{m \in \text{method}} n_{\text{met}}(m) + n_{\text{var}})}
    \end{align*}

    $\mathbf{P}(\textbf{expression} | \text{form}, \text{type}, \text{context})$ depends on the form:
    \begin{itemize}
      \item $P(x | \text{form}=\texttt{variable}, \text{type}, \text{context})$ is a uniform distribution over all variables in scope
      \item $P(l | \text{form}=\texttt{literal}, \text{type}, \text{context}) = \frac{n_{\text{lit}}(l)}{\Sigma_{l \in \text{literal}} n_{\text{lit}}(l)}$
    \end{itemize}
  }

  \headerbox{Results}{name=results, column=2, row=0}{
    We tested our predictor using leave-one-out cross validation on blah blah corpuses, and measured the average probability of the correct expression.

    Here is a pretty graph showing the accuracy of our model vs n-gram on a few different projects:
    \begin{displaymath}
      \boxed{
        \xymatrix{
          - & & - \\
           & - & \\
          -\ar[uu] & - \ar[u] & -\ar[uu]
        }
    }
    \end{displaymath}
  }

  \headerbox{Conclusions}{name=conclusion, column=2, below=results}{
    Our tests show that our method performs significantly better than the n-gram model. There are several avenues for future work:

    \begin{itemize}
      \firstitem Handle the rest of Java
      \item Make this a proper IDE plugin
      \item more research 1
      \item more research 2
    \end{itemize}
  }

  \end{poster}
  \end{document}
