\documentclass[landscape,final,a0paper,fontscale=0.297]{baposter}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}

\usepackage{multicol}

\usepackage{xypic}

% Multicol Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\columnsep}{1.5em}
\setlength{\columnseprule}{0mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Save space in lists. Use this after the opening of the list
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\compresslist}{%
\setlength{\itemsep}{1pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

\newcommand{\firstitem}{\item\vskip-5pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Here starts the poster
%%%---------------------------------------------------------------------------
%%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define some colors

\begin{poster}%
  % Poster Options
  {
  % Show grid to help with alignment
  grid=false,
  columns=3,
  % Column spacing
  colspacing=1em,
  % Color style
  bgColorOne=white,
  bgColorTwo=white,
  borderColor=black,
  headerColorOne=cyan,
  headerColorTwo=white,
  headerFontColor=black,
  boxColorOne=white,
  boxColorTwo=white,
  % Format of textbox
  textborder=rounded,
  % Format of text header
  eyecatcher=true,
  headerborder=closed,
  headerheight=0.1\textheight,
%  textfont=\sc, An example of changing the text font
  headershape=rounded,
  headershade=plain,
  headerfont=\Large\bf\textsc, %Sans Serif
  textfont={\setlength{\parindent}{1.5em}},
  boxshade=plain,
%  background=shade-tb,
  background=plain,
  linewidth=1pt
  }
  % Eye Catcher
  {} 
  % Title
  {\bf\textsc{Structured Statistical Source Code Prediction}\vspace{0.5em}}
  % Authors
  {\textsc{Cyrus Omar, Flavio Cruz, Salil Joshi}}
  % University logo
  {}
  %{% The makebox allows the title to flow into the logo, this is a hack because of the L shaped logo.
  %  \includegraphics[height=9.0em]{images/logo}
  %}

\newcommand{\asgn}{\textbf{assignment}}
\newcommand{\statement}{\textbf{statement}}
\newcommand{\argument}{\textbf{argument}}
\newcommand{\other}{\textbf{other}}

\newcommand{\lit}{\textbf{lit}}
\newcommand{\meth}{\textbf{meth}}
\newcommand{\var}{\textbf{var}}

\newcommand{\nm}[1]{\#\{#1\}}

  \headerbox{Introduction}{name=intro, column=0, row = 0}{
  \noindent
    %Our goal is to predict source code using both the \emph{semantics} of the programming language, and \emph{statistics} derived from code corpuses. 
    We develop a model that assigns probabilities to expressions, $e$, in the Java programming language. We represented expressions as syntax trees and develop a factored model based on semantic constraints imposed by the Java language, parameterizing it with data gathered from a corpus of open source Java projects.
    \begin{itemize}\compresslist        
      \item \textbf{Semantic Constraints}:
      \item[--] We use the \emph{type} of the expression, $\tau$, to constrain our prediction space. This is similar to, but more general than, what is done by IDE code completion systems that produce unordered lists of members.
      \item[--] We condition our predictions on the \emph{syntactic context}, $\Sigma$, i.e.\ the AST structure in which the expression appears. For example, in $\texttt{int i = } \Box$ the hole ($\Box$) appears in an assignment context. We consider four kinds of syntactic contexts, $\Sigma \in \{\statement, \asgn, \argument, \other\}$.
      \item[--] We increase the accuracy of our prediction by using the \emph{variable context}, $\Gamma$, i.e. the set of available variables and their types -- at the point in the program that the expression appears.
    \end{itemize}

\noindent
Code prediction is useful for code editors, but also for other specialized tools like enabling programmers with physical impairments to convey source code using input devices more limited than a keyboard
  }


  \headerbox{Model}{name=model, column=0, below = intro}{
  \noindent
      Our goal is to learn a model that allows us to produce $\mathbf{P}(e|\tau, \Sigma, \Gamma)$. We factor this by  marginalizing over a variable representing the \emph{syntactic form} of the expression,  which will allow us to predict the expression more easily:
    $$P(e | \tau, \Sigma, \Gamma) = \sum_{\phi \in \Phi} P(e | \phi, \tau, \Sigma, \Gamma) P(\phi | \tau, \Sigma) $$
	\noindent
	The syntactic forms we consider are $\Phi = \{\lit, \meth, \var\}$, representing literals (built-in and enumerations), method calls and variables, respectively. We can thus draw our model as follows:
    \begin{displaymath}
    \xymatrix{
      *+[F]{\color{black}\txt{Type ($\tau$)}} \ar[dr] \ar[drr] & &\\
      & *+[F]{\color{black} \txt{Syntactic\\Form ($\phi$)}} \ar[r] & *+[F]{\color{cyan}\txt{Expression ($e$)}}  \\
      *+[F]{\color{black}\txt{Syntactic\\Context ($\Sigma$)}} \ar[ur] \ar[urr] & & *+[F]{\color{black}\txt{Variable\\Context ($\Gamma$)}} \ar[u]  
    }
    \end{displaymath}
    \\
  }
  \headerbox{Prediction}{name=prediction, column=1, row=0}{
  \noindent
  We use the notation $\nm{...}$ to represent the number of expressions in the training set constrained by the provided expression, type, form and syntactic context (summing over any omitted categories.)
  
  
  The conditional distribution for the syntactic forms is categorical, with the probability for each $\phi \in \Phi$ learned as:
  $$P(\phi | \tau, \Sigma) = \frac{\nm{\phi,\tau, \Sigma}}{\nm{\tau, \Sigma}}$$
  
  The conditional distribution for an expression $e$, given its actual syntactic form, $\phi_e$, is:
$$P(e | \phi, \tau, \Sigma, \Gamma) =
\left\{
	\begin{array}{ll}
		0  & \mbox{if } \phi \neq \phi_e \\
		P_{\lit}(e | \tau, \Sigma, \Gamma) & \mbox{if } \phi = \phi_e = \lit\\
		P_{\var}(e | \tau, \Sigma, \Gamma) & \mbox{if } \phi = \phi_e = \var\\
		P_{\meth}(e | \tau, \Sigma, \Gamma) & \mbox{if } \phi = \phi_e = \meth
	\end{array}
\right.
$$

where
$$P_{\lit}(e | \tau, \Sigma, \Gamma) = \frac{\nm{e, \tau, \Sigma}}{\nm{\lit, \tau, \Sigma}}$$
$$P_{\var}(e | \tau, \Sigma, \Gamma) = 
\left\{
	\begin{array}{ll}
	\frac{1}{|V(\Gamma, \tau)|} & e : \tau \in \Gamma\\
	0 & \mbox{o/w}
	\end{array}
\right.$$
$$P_{\meth}(e | \tau, \Sigma, \Gamma) = 
\left\{
	\begin{array}{ll}
	\left(P_{\text{seen}}(\tau, \Sigma)\frac{\nm{(\tau_0, \ldots, \tau_n \rightarrow \tau), \Sigma}}{\nm{\meth, \tau, \Sigma}}\right. & \mathcal{M}(e, \tau_0, \ldots, \tau_n, \tau) \\
	  \left. + (1 - P_{\text{seen}}(\tau, \Sigma))\frac{1}{|M(\Gamma, \tau)|}\right) \times & \\
	  \prod_{i=1}^n P(e_i | \tau_i, \argument, \Gamma)& \\
	0 & \mbox{o/w}
	\end{array}
\right.$$
$$P_{\text{seen}}(\tau, \Sigma) = \frac{|\nm{\meth, \tau, \Sigma} = 1|}{\nm{\meth, \tau, \Sigma}}$$
    }
    
%    \Gamma \vdash o : \tau_0
%\land~\Gamma \vdash e_1 : \tau_1~\cdots~\Gamma \vdash e_n : \tau_n
%\land~\Gamma \vdash \tau_0.m : \tau_1, \ldots, \tau_n \rightarrow \tau
  \headerbox{Learning}{name=learning, column=1, below=prediction}{
    \setlength{\parindent}{0pt}
    We analyze existing code to learn the following parameters:
    \begin{itemize}
      \item $\mathbf{n}_{\mathbf{lit}}(l)$: For a literal $l$, $\#\{l | \text{type}, \text{context}\}$ i.e.\ the number of times $l$ was used in a given type and context
      \item $\mathbf{n}_{\mathbf{met}}(m)$ For a method $m$, $\#\{m | \text{type}, \text{context}\}$ i.e.\ the number of times $m$ was used in a given type and context
      \item $\mathbf{n}_{\mathbf{var}}$: $\#\{\text{variable} | \text{type}, \text{context}\}$ i.e.\ the number of times \emph{any} variable was used in a given type and context
      \item $\#\{\text{first use of a method } | \text{type}, \text{context}\}$ this is the number of times a previously unseen method was used.
      \item $\#\{\text{methods} | \text{type}\}$ i.e.\ the number of methods (used and unused) with the appropriate return type
    \end{itemize}
  }


  \headerbox{Results}{name=results, column=2, row=0}{
    We tested our predictor using leave-one-out cross validation on blah blah corpuses, and measured the average probability of the correct expression.

    Here is a pretty graph showing the accuracy of our model vs n-gram on a few different projects:
    \begin{displaymath}
      \boxed{
        \xymatrix{
          - & & - \\
           & - & \\
          -\ar[uu] & - \ar[u] & -\ar[uu]
        }
    }
    \end{displaymath}
  }

  \headerbox{Conclusions}{name=conclusion, column=2, below=results}{
    Our tests show that our method performs significantly better than the n-gram model. There are several avenues for future work:

    \begin{itemize}
      \firstitem Handle the rest of Java
      \item Make this a proper IDE plugin
      \item more research 1
      \item more research 2
    \end{itemize}
  }

  \end{poster}
  \end{document}
